This project explores automatic image colorization by converting grayscale images into realistic color images using a U-Netâ€“based image-to-image translation model. Trained on the STL-10 dataset, the model learns to map single-channel grayscale inputs to RGB outputs through paired supervision. Two training strategies were compared: a pixel-wise L1 loss and a combined L1 plus perceptual loss using VGG16 features. Results show a trade-off between visual richness and accuracy, where perceptual loss produces more saturated and visually appealing images, while L1 loss yields more conservative and accurate color predictions. The findings highlight how loss function choice influences confidence, saturation, and realism in neural image colorization.
